<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>src.gan.discriminator API documentation</title>
<meta name="description" content="Implement the Discriminator part of the ArtGAN â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.gan.discriminator</code></h1>
</header>
<section id="section-intro">
<p>Implement the Discriminator part of the ArtGAN.</p>
<p>The purpose of this file is to properly define the discriminative part
of the ArtGAN, as described in the paper. To do so, we separate the Enc
and the clsNet which are then combined to make the Discriminator.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Implement the Discriminator part of the ArtGAN.

The purpose of this file is to properly define the discriminative part
of the ArtGAN, as described in the paper. To do so, we separate the Enc
and the clsNet which are then combined to make the Discriminator.
&#34;&#34;&#34;


# Importing Python packages
import os
import sys 
path = os.path.dirname(os.path.abspath(__file__))
sys.path.append(path)
import torch
import torch.nn as nn


class Enc(nn.Module):
    &#34;&#34;&#34;The first part of the Discriminator, namely the Encoder.&#34;&#34;&#34;

    def __init__(self, input_channels: int = 3, alpha: float = 0.2):
        &#34;&#34;&#34;Initialize the Encoder par of the Discriminator.

        It takes the input image or a fake image
        and runs several convolutional layers on it in order to
        create features that can be sent to the following classifier

        Args:
            input_channels: the number of channels in the input data
                e.g. 3 for RGB channels
            alpha: positive coefficient to be used as the parameter
                for leakyReLU activation e.g. 0.2 as in the original paper
        &#34;&#34;&#34;
        super(Enc, self).__init__()

        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=128, kernel_size=4, stride=2, padding=1)
        self.conv1lrelu = nn.LeakyReLU(negative_slope=alpha, inplace=True)

        self.conv2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)
        self.conv2bn = nn.BatchNorm2d(num_features=128, affine=True)
        self.conv2lrelu = nn.LeakyReLU(negative_slope=alpha, inplace=True)

        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1)
        self.conv3bn = nn.BatchNorm2d(num_features=256, affine=True)
        self.conv3lrelu = nn.LeakyReLU(negative_slope=alpha, inplace=True)

        self.conv4 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1)
        self.conv4bn = nn.BatchNorm2d(num_features=512, affine=True)
        self.conv4lrelu = nn.LeakyReLU(negative_slope=alpha, inplace=True)

    def forward(self, image: torch.FloatTensor) -&gt; torch.FloatTensor:
        &#34;&#34;&#34;Compute the forward propagation on input data.

        Args:
            image: a torch tensor of dimension (N, C, H, W) where
                C is equal to input_channels from initialization e.g. 3 for RGB

        Returns:
            enc_img: the result of the propagation
        &#34;&#34;&#34;
        enc_img = self.conv1(image)
        enc_img = self.conv1lrelu(enc_img)

        enc_img = self.conv2(enc_img)
        enc_img = self.conv2bn(enc_img)
        enc_img = self.conv2lrelu(enc_img)

        enc_img = self.conv3(enc_img)
        enc_img = self.conv3bn(enc_img)
        enc_img = self.conv3lrelu(enc_img)

        enc_img = self.conv4(enc_img)
        enc_img = self.conv4bn(enc_img)
        enc_img = self.conv4lrelu(enc_img)

        return enc_img


class clsNet(nn.Module):
    &#34;&#34;&#34;Implement the clsNet which will classify the data.&#34;&#34;&#34;

    def __init__(self, img_size: int, nb_classes: int, alpha: float = 0.2):
        &#34;&#34;&#34;Initialize the clsNet.

        This is a classifier that takes the transformed data from Encoder
        and tries to find the class of the input image, which is
        in a set of K classes plus a FAKE class

        Args:
            img_size: the size of the image, which is supposed to be
                a square image of size img_size*img_size*input_channels
            nb_classes: the number of classes for the classifier
                to choose from excepting FAKE class e.g. 10 for CIFAR-10
            alpha: positive coefficient for the leakyReLU
                activation for convolutional layer(s) e.g. 0.2 as in the
                original paper
        &#34;&#34;&#34;
        super(clsNet, self).__init__()

        self.conv5 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=4, stride=2, padding=1)
        self.conv5bn = nn.BatchNorm2d(num_features=1024, affine=True)
        self.conv5lrelu = nn.LeakyReLU(negative_slope=alpha, inplace=True)

        self.fc6 = nn.Linear(in_features=(img_size//16)*(img_size//16)*1024, out_features=nb_classes+1)
        self.fc6sig = nn.Sigmoid()

    def forward(self, enc_img: torch.FloatTensor) -&gt; torch.FloatTensor:
        &#34;&#34;&#34;Compute the forward propagation of this network on input data.

        Args:
            enc_img: a torch tensor of dimensions (N, C, H, W)
                where C = 512

        Returns:
            clas: the predicted classes for this data
        &#34;&#34;&#34;
        clas = self.conv5(enc_img)
        clas = self.conv5bn(clas)
        clas = self.conv5lrelu(clas)

        clas = clas.view(clas.size(0), -1)
        clas = self.fc6sig(self.fc6(clas))

        return clas


class Discriminator(nn.Module):
    &#34;&#34;&#34;Gather the two parts of the Discriminator into one class.&#34;&#34;&#34;

    def __init__(self, img_size: int, nb_classes: int, input_channels: int = 3, alpha: float = 0.2):
        &#34;&#34;&#34;Initialize the Discriminator part of the GAN.

        It is meant to treat an input image and classify it whether in
        a FAKE class or in its best class.

        Args:
            img_size: the size of the input image, which is expected to
                be a squared image of size img_size*img_size*input_channels
            nb_classes: the number of classes in the input images,
                except FAKE class e.g. 10 for CIFAR-10
            input_channels: the number of channels in the input images,
                e.g. 3 for RGB channels
            alpha: positive coefficient for the slope in the
                leakyReLU activation e.g. 0.2 as in the original paper
        &#34;&#34;&#34;
        super(Discriminator, self).__init__()

        self.enc = Enc(input_channels=input_channels, alpha=alpha)
        self.clsnet = clsNet(img_size, nb_classes, alpha=alpha)

    def forward(self, img: torch.FloatTensor) -&gt; torch.FloatTensor:
        &#34;&#34;&#34;Compute the forward propagation on input images.

        Args:
            img: a torch tensor containing images of dimension
                (N, C, H, W) where C = nb_classes, H = img_size &amp; W = img_size from initialization
                parameters

        Returns:
            Y: the predicted classes for the image
        &#34;&#34;&#34;
        Y = self.enc(img)
        Y = self.clsnet(Y)

        return Y

    def encode(self, img: torch.FloatTensor) -&gt; torch.FloatTensor:
        &#34;&#34;&#34;Encode an input image into features.

        Args:
            img: a torch tensor containing images of
                dimension (N, C, H, W) where C = nb_classes, H = img_size &amp; W = img_size
                from initialization

        Returns:
            enc_img: the result of encoding
        &#34;&#34;&#34;
        enc_img = self.enc(img)

        return enc_img
        </code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="src.gan.discriminator.Discriminator"><code class="flex name class">
<span>class <span class="ident">Discriminator</span></span>
<span>(</span><span>img_size:Â int, nb_classes:Â int, input_channels:Â intÂ =Â 3, alpha:Â floatÂ =Â 0.2)</span>
</code></dt>
<dd>
<div class="desc"><p>Gather the two parts of the Discriminator into one class.</p>
<p>Initialize the Discriminator part of the GAN.</p>
<p>It is meant to treat an input image and classify it whether in
a FAKE class or in its best class.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>img_size</code></strong></dt>
<dd>the size of the input image, which is expected to
be a squared image of size img_size<em>img_size</em>input_channels</dd>
<dt><strong><code>nb_classes</code></strong></dt>
<dd>the number of classes in the input images,
except FAKE class e.g. 10 for CIFAR-10</dd>
<dt><strong><code>input_channels</code></strong></dt>
<dd>the number of channels in the input images,
e.g. 3 for RGB channels</dd>
<dt><strong><code>alpha</code></strong></dt>
<dd>positive coefficient for the slope in the
leakyReLU activation e.g. 0.2 as in the original paper</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Discriminator(nn.Module):
    &#34;&#34;&#34;Gather the two parts of the Discriminator into one class.&#34;&#34;&#34;

    def __init__(self, img_size: int, nb_classes: int, input_channels: int = 3, alpha: float = 0.2):
        &#34;&#34;&#34;Initialize the Discriminator part of the GAN.

        It is meant to treat an input image and classify it whether in
        a FAKE class or in its best class.

        Args:
            img_size: the size of the input image, which is expected to
                be a squared image of size img_size*img_size*input_channels
            nb_classes: the number of classes in the input images,
                except FAKE class e.g. 10 for CIFAR-10
            input_channels: the number of channels in the input images,
                e.g. 3 for RGB channels
            alpha: positive coefficient for the slope in the
                leakyReLU activation e.g. 0.2 as in the original paper
        &#34;&#34;&#34;
        super(Discriminator, self).__init__()

        self.enc = Enc(input_channels=input_channels, alpha=alpha)
        self.clsnet = clsNet(img_size, nb_classes, alpha=alpha)

    def forward(self, img: torch.FloatTensor) -&gt; torch.FloatTensor:
        &#34;&#34;&#34;Compute the forward propagation on input images.

        Args:
            img: a torch tensor containing images of dimension
                (N, C, H, W) where C = nb_classes, H = img_size &amp; W = img_size from initialization
                parameters

        Returns:
            Y: the predicted classes for the image
        &#34;&#34;&#34;
        Y = self.enc(img)
        Y = self.clsnet(Y)

        return Y

    def encode(self, img: torch.FloatTensor) -&gt; torch.FloatTensor:
        &#34;&#34;&#34;Encode an input image into features.

        Args:
            img: a torch tensor containing images of
                dimension (N, C, H, W) where C = nb_classes, H = img_size &amp; W = img_size
                from initialization

        Returns:
            enc_img: the result of encoding
        &#34;&#34;&#34;
        enc_img = self.enc(img)

        return enc_img</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="src.gan.discriminator.Discriminator.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="src.gan.discriminator.Discriminator.training"><code class="name">var <span class="ident">training</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="src.gan.discriminator.Discriminator.encode"><code class="name flex">
<span>def <span class="ident">encode</span></span>(<span>self, img:Â torch.FloatTensor) â€‘>Â torch.FloatTensor</span>
</code></dt>
<dd>
<div class="desc"><p>Encode an input image into features.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>img</code></strong></dt>
<dd>a torch tensor containing images of
dimension (N, C, H, W) where C = nb_classes, H = img_size &amp; W = img_size
from initialization</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>enc_img</code></dt>
<dd>the result of encoding</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def encode(self, img: torch.FloatTensor) -&gt; torch.FloatTensor:
    &#34;&#34;&#34;Encode an input image into features.

    Args:
        img: a torch tensor containing images of
            dimension (N, C, H, W) where C = nb_classes, H = img_size &amp; W = img_size
            from initialization

    Returns:
        enc_img: the result of encoding
    &#34;&#34;&#34;
    enc_img = self.enc(img)

    return enc_img</code></pre>
</details>
</dd>
<dt id="src.gan.discriminator.Discriminator.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, img:Â torch.FloatTensor) â€‘>Â torch.FloatTensor</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the forward propagation on input images.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>img</code></strong></dt>
<dd>a torch tensor containing images of dimension
(N, C, H, W) where C = nb_classes, H = img_size &amp; W = img_size from initialization
parameters</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Y</code></dt>
<dd>the predicted classes for the image</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, img: torch.FloatTensor) -&gt; torch.FloatTensor:
    &#34;&#34;&#34;Compute the forward propagation on input images.

    Args:
        img: a torch tensor containing images of dimension
            (N, C, H, W) where C = nb_classes, H = img_size &amp; W = img_size from initialization
            parameters

    Returns:
        Y: the predicted classes for the image
    &#34;&#34;&#34;
    Y = self.enc(img)
    Y = self.clsnet(Y)

    return Y</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="src.gan.discriminator.Enc"><code class="flex name class">
<span>class <span class="ident">Enc</span></span>
<span>(</span><span>input_channels:Â intÂ =Â 3, alpha:Â floatÂ =Â 0.2)</span>
</code></dt>
<dd>
<div class="desc"><p>The first part of the Discriminator, namely the Encoder.</p>
<p>Initialize the Encoder par of the Discriminator.</p>
<p>It takes the input image or a fake image
and runs several convolutional layers on it in order to
create features that can be sent to the following classifier</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_channels</code></strong></dt>
<dd>the number of channels in the input data
e.g. 3 for RGB channels</dd>
<dt><strong><code>alpha</code></strong></dt>
<dd>positive coefficient to be used as the parameter
for leakyReLU activation e.g. 0.2 as in the original paper</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Enc(nn.Module):
    &#34;&#34;&#34;The first part of the Discriminator, namely the Encoder.&#34;&#34;&#34;

    def __init__(self, input_channels: int = 3, alpha: float = 0.2):
        &#34;&#34;&#34;Initialize the Encoder par of the Discriminator.

        It takes the input image or a fake image
        and runs several convolutional layers on it in order to
        create features that can be sent to the following classifier

        Args:
            input_channels: the number of channels in the input data
                e.g. 3 for RGB channels
            alpha: positive coefficient to be used as the parameter
                for leakyReLU activation e.g. 0.2 as in the original paper
        &#34;&#34;&#34;
        super(Enc, self).__init__()

        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=128, kernel_size=4, stride=2, padding=1)
        self.conv1lrelu = nn.LeakyReLU(negative_slope=alpha, inplace=True)

        self.conv2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)
        self.conv2bn = nn.BatchNorm2d(num_features=128, affine=True)
        self.conv2lrelu = nn.LeakyReLU(negative_slope=alpha, inplace=True)

        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1)
        self.conv3bn = nn.BatchNorm2d(num_features=256, affine=True)
        self.conv3lrelu = nn.LeakyReLU(negative_slope=alpha, inplace=True)

        self.conv4 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1)
        self.conv4bn = nn.BatchNorm2d(num_features=512, affine=True)
        self.conv4lrelu = nn.LeakyReLU(negative_slope=alpha, inplace=True)

    def forward(self, image: torch.FloatTensor) -&gt; torch.FloatTensor:
        &#34;&#34;&#34;Compute the forward propagation on input data.

        Args:
            image: a torch tensor of dimension (N, C, H, W) where
                C is equal to input_channels from initialization e.g. 3 for RGB

        Returns:
            enc_img: the result of the propagation
        &#34;&#34;&#34;
        enc_img = self.conv1(image)
        enc_img = self.conv1lrelu(enc_img)

        enc_img = self.conv2(enc_img)
        enc_img = self.conv2bn(enc_img)
        enc_img = self.conv2lrelu(enc_img)

        enc_img = self.conv3(enc_img)
        enc_img = self.conv3bn(enc_img)
        enc_img = self.conv3lrelu(enc_img)

        enc_img = self.conv4(enc_img)
        enc_img = self.conv4bn(enc_img)
        enc_img = self.conv4lrelu(enc_img)

        return enc_img</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="src.gan.discriminator.Enc.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="src.gan.discriminator.Enc.training"><code class="name">var <span class="ident">training</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="src.gan.discriminator.Enc.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, image:Â torch.FloatTensor) â€‘>Â torch.FloatTensor</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the forward propagation on input data.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>image</code></strong></dt>
<dd>a torch tensor of dimension (N, C, H, W) where
C is equal to input_channels from initialization e.g. 3 for RGB</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>enc_img</code></dt>
<dd>the result of the propagation</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, image: torch.FloatTensor) -&gt; torch.FloatTensor:
    &#34;&#34;&#34;Compute the forward propagation on input data.

    Args:
        image: a torch tensor of dimension (N, C, H, W) where
            C is equal to input_channels from initialization e.g. 3 for RGB

    Returns:
        enc_img: the result of the propagation
    &#34;&#34;&#34;
    enc_img = self.conv1(image)
    enc_img = self.conv1lrelu(enc_img)

    enc_img = self.conv2(enc_img)
    enc_img = self.conv2bn(enc_img)
    enc_img = self.conv2lrelu(enc_img)

    enc_img = self.conv3(enc_img)
    enc_img = self.conv3bn(enc_img)
    enc_img = self.conv3lrelu(enc_img)

    enc_img = self.conv4(enc_img)
    enc_img = self.conv4bn(enc_img)
    enc_img = self.conv4lrelu(enc_img)

    return enc_img</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="src.gan.discriminator.clsNet"><code class="flex name class">
<span>class <span class="ident">clsNet</span></span>
<span>(</span><span>img_size:Â int, nb_classes:Â int, alpha:Â floatÂ =Â 0.2)</span>
</code></dt>
<dd>
<div class="desc"><p>Implement the clsNet which will classify the data.</p>
<p>Initialize the clsNet.</p>
<p>This is a classifier that takes the transformed data from Encoder
and tries to find the class of the input image, which is
in a set of K classes plus a FAKE class</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>img_size</code></strong></dt>
<dd>the size of the image, which is supposed to be
a square image of size img_size<em>img_size</em>input_channels</dd>
<dt><strong><code>nb_classes</code></strong></dt>
<dd>the number of classes for the classifier
to choose from excepting FAKE class e.g. 10 for CIFAR-10</dd>
<dt><strong><code>alpha</code></strong></dt>
<dd>positive coefficient for the leakyReLU
activation for convolutional layer(s) e.g. 0.2 as in the
original paper</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class clsNet(nn.Module):
    &#34;&#34;&#34;Implement the clsNet which will classify the data.&#34;&#34;&#34;

    def __init__(self, img_size: int, nb_classes: int, alpha: float = 0.2):
        &#34;&#34;&#34;Initialize the clsNet.

        This is a classifier that takes the transformed data from Encoder
        and tries to find the class of the input image, which is
        in a set of K classes plus a FAKE class

        Args:
            img_size: the size of the image, which is supposed to be
                a square image of size img_size*img_size*input_channels
            nb_classes: the number of classes for the classifier
                to choose from excepting FAKE class e.g. 10 for CIFAR-10
            alpha: positive coefficient for the leakyReLU
                activation for convolutional layer(s) e.g. 0.2 as in the
                original paper
        &#34;&#34;&#34;
        super(clsNet, self).__init__()

        self.conv5 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=4, stride=2, padding=1)
        self.conv5bn = nn.BatchNorm2d(num_features=1024, affine=True)
        self.conv5lrelu = nn.LeakyReLU(negative_slope=alpha, inplace=True)

        self.fc6 = nn.Linear(in_features=(img_size//16)*(img_size//16)*1024, out_features=nb_classes+1)
        self.fc6sig = nn.Sigmoid()

    def forward(self, enc_img: torch.FloatTensor) -&gt; torch.FloatTensor:
        &#34;&#34;&#34;Compute the forward propagation of this network on input data.

        Args:
            enc_img: a torch tensor of dimensions (N, C, H, W)
                where C = 512

        Returns:
            clas: the predicted classes for this data
        &#34;&#34;&#34;
        clas = self.conv5(enc_img)
        clas = self.conv5bn(clas)
        clas = self.conv5lrelu(clas)

        clas = clas.view(clas.size(0), -1)
        clas = self.fc6sig(self.fc6(clas))

        return clas</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="src.gan.discriminator.clsNet.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="src.gan.discriminator.clsNet.training"><code class="name">var <span class="ident">training</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="src.gan.discriminator.clsNet.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, enc_img:Â torch.FloatTensor) â€‘>Â torch.FloatTensor</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the forward propagation of this network on input data.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>enc_img</code></strong></dt>
<dd>a torch tensor of dimensions (N, C, H, W)
where C = 512</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>clas</code></dt>
<dd>the predicted classes for this data</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, enc_img: torch.FloatTensor) -&gt; torch.FloatTensor:
    &#34;&#34;&#34;Compute the forward propagation of this network on input data.

    Args:
        enc_img: a torch tensor of dimensions (N, C, H, W)
            where C = 512

    Returns:
        clas: the predicted classes for this data
    &#34;&#34;&#34;
    clas = self.conv5(enc_img)
    clas = self.conv5bn(clas)
    clas = self.conv5lrelu(clas)

    clas = clas.view(clas.size(0), -1)
    clas = self.fc6sig(self.fc6(clas))

    return clas</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src.gan" href="index.html">src.gan</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="src.gan.discriminator.Discriminator" href="#src.gan.discriminator.Discriminator">Discriminator</a></code></h4>
<ul class="">
<li><code><a title="src.gan.discriminator.Discriminator.dump_patches" href="#src.gan.discriminator.Discriminator.dump_patches">dump_patches</a></code></li>
<li><code><a title="src.gan.discriminator.Discriminator.encode" href="#src.gan.discriminator.Discriminator.encode">encode</a></code></li>
<li><code><a title="src.gan.discriminator.Discriminator.forward" href="#src.gan.discriminator.Discriminator.forward">forward</a></code></li>
<li><code><a title="src.gan.discriminator.Discriminator.training" href="#src.gan.discriminator.Discriminator.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.gan.discriminator.Enc" href="#src.gan.discriminator.Enc">Enc</a></code></h4>
<ul class="">
<li><code><a title="src.gan.discriminator.Enc.dump_patches" href="#src.gan.discriminator.Enc.dump_patches">dump_patches</a></code></li>
<li><code><a title="src.gan.discriminator.Enc.forward" href="#src.gan.discriminator.Enc.forward">forward</a></code></li>
<li><code><a title="src.gan.discriminator.Enc.training" href="#src.gan.discriminator.Enc.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.gan.discriminator.clsNet" href="#src.gan.discriminator.clsNet">clsNet</a></code></h4>
<ul class="">
<li><code><a title="src.gan.discriminator.clsNet.dump_patches" href="#src.gan.discriminator.clsNet.dump_patches">dump_patches</a></code></li>
<li><code><a title="src.gan.discriminator.clsNet.forward" href="#src.gan.discriminator.clsNet.forward">forward</a></code></li>
<li><code><a title="src.gan.discriminator.clsNet.training" href="#src.gan.discriminator.clsNet.training">training</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>