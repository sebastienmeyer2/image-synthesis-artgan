<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>src.gan.generator API documentation</title>
<meta name="description" content="Implement the Generator part of the ArtGAN …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.gan.generator</code></h1>
</header>
<section id="section-intro">
<p>Implement the Generator part of the ArtGAN.</p>
<p>The purpose of this file is to properly define the generative part
of the ArtGAN as described in the paper. To do so, we separate the
zNet and the Dec, which are then combined to make the Generator class.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Implement the Generator part of the ArtGAN.

The purpose of this file is to properly define the generative part
of the ArtGAN as described in the paper. To do so, we separate the
zNet and the Dec, which are then combined to make the Generator class.
&#34;&#34;&#34;


# Importing Python packages
import os
import sys 
path = os.path.dirname(os.path.abspath(__file__))
sys.path.append(path)
import torch
import torch.nn as nn


class zNet(nn.Module):
    &#34;&#34;&#34;Implement the module converting dense features to latent features.&#34;&#34;&#34;

    def __init__(self, img_size: int, start_channels: int = 110):
        &#34;&#34;&#34;Initialize the zNet network.

        This is the first part of the Generator. Hence, its task is
        to convert the dense code into a latent code to be used in the
        Decoder

        Args:
            img_size: the size of the output images to be created
                at the end of the Generator, which are expected to be
                squared images of size img_size*img_size*input_channels where
                input_channels is from the Discriminator initialization (not
                this one)
            start_channels: the number of channels in
                the input data

        Raises:
            ValueError if the size of image is less than 64 due to
            the operations executed by the deconv layers
        &#34;&#34;&#34;
        super(zNet, self).__init__()

        # Parameters

        if img_size &lt; 64:
            raise ValueError(&#34;The size of the input images has to be of at least 64 pixels!&#34;)

        self.start_channels = start_channels
        self.img_size = img_size

        # Deconvolution layers

        self.deconv1 = nn.ConvTranspose2d(in_channels=start_channels, out_channels=1024, kernel_size=4, stride=1, padding=0)
        self.deconv1bn = nn.BatchNorm2d(num_features=1024, affine=True)
        self.deconv1relu = nn.ReLU(inplace=False)

        self.deconv2 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=4, stride=2, padding=1)
        self.deconv2bn = nn.BatchNorm2d(num_features=512, affine=True)
        self.deconv2relu = nn.ReLU(inplace=False)

    def forward(self, dense: torch.FloatTensor) -&gt; torch.FloatTensor:
        &#34;&#34;&#34;Compute the propagation of this network on input data.

        Args:
            dense: a torch tensor of dimension
                (N, C, H, W)

        Returns:
            latent: the result of propagation with this
                network
        &#34;&#34;&#34;
        # We need to ensure that the data has correct size, for this we used the formula to
        # calculate the result of convolutions (reversed)
        latent = dense.view(dense.size(0), self.start_channels, (self.img_size//16)-3, (self.img_size//16)-3)
        latent = self.deconv1(latent)
        latent = self.deconv1bn(latent)
        latent = self.deconv1relu(latent)

        latent = self.deconv2(latent)
        latent = self.deconv2bn(latent)
        latent = self.deconv2relu(latent)

        return latent


class Dec(nn.Module):
    &#34;&#34;&#34;Implement the Decoder part of the Generator.&#34;&#34;&#34;

    def __init__(self):
        &#34;&#34;&#34;Initialize the Decoder (or Dec).

        This is the second part of the Generator. Hence, its task is
        to work with the latent code and generate a fake image to be sent
        to the Discriminator
        &#34;&#34;&#34;
        super(Dec, self).__init__()

        self.deconv3 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=4, stride=2, padding=1)
        self.deconv3bn = nn.BatchNorm2d(num_features=256, affine=True)
        self.deconv3dropout = nn.Dropout(p=0.5)
        self.deconv3relu = nn.ReLU(inplace=False)

        self.deconv4 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1)
        self.deconv4bn = nn.BatchNorm2d(num_features=128, affine=True)
        self.deconv4dropout = nn.Dropout(p=0.5)
        self.deconv4relu = nn.ReLU(inplace=False)

        self.deconv5 = nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)
        self.deconv5bn = nn.BatchNorm2d(num_features=128, affine=True)
        self.deconv5relu = nn.ReLU(inplace=False)

        self.deconv6 = nn.ConvTranspose2d(in_channels=128, out_channels=3, kernel_size=4, stride=2, padding=1)
        self.deconv6sig = nn.Sigmoid()

    def forward(self, latent: torch.FloatTensor) -&gt; torch.FloatTensor:
        &#34;&#34;&#34;Compute the forward propagation of this network on input data.

        Args:
            latent: a torch tensor of dimension (N, C, H, W)
                to be treated

        Returns:
            dec_img: the result of the propagation with this network
        &#34;&#34;&#34;
        dec_img = self.deconv3(latent)
        dec_img = self.deconv3bn(dec_img)
        dec_img = self.deconv3dropout(dec_img)
        dec_img = self.deconv3relu(dec_img)

        dec_img = self.deconv4(dec_img)
        dec_img = self.deconv4bn(dec_img)
        dec_img = self.deconv4dropout(dec_img)
        dec_img = self.deconv4relu(dec_img)

        dec_img = self.deconv5(dec_img)
        dec_img = self.deconv5bn(dec_img)
        dec_img = self.deconv5relu(dec_img)

        dec_img = self.deconv6(dec_img)
        dec_img = self.deconv6sig(dec_img)

        return dec_img


class Generator(nn.Module):
    &#34;&#34;&#34;Gather the two parts of the Generator in one class.&#34;&#34;&#34;

    def __init__(self, img_size: int, start_channels: int = 110):
        &#34;&#34;&#34;Initialize the complete generator part of the GAN.

        Args:
            img_size: the size of the output images to be built
                and sent to the Discriminator, which are expected to be
                squared images of size img_size*img_size*input_channels
                where input_channels is used for the Discriminator&#39;s
                initialization
            start_channels: the number of channels in the
                input images, free to choose

        Raises:
            ValueError if the value of img_size is less than 64 pixels due
            to the operations executed by the deconv layers
        &#34;&#34;&#34;
        if img_size &lt; 64:
            raise ValueError(&#34;The size of the input images has to be of at least 64 pixels!&#34;)

        super(Generator, self).__init__()

        self.znet = zNet(img_size, start_channels=start_channels)
        self.dec = Dec()

    def forward(self, dense: torch.FloatTensor) -&gt; torch.FloatTensor:
        &#34;&#34;&#34;Compute the forward propagation on input data.

        Args:
            dense: a torch tensor of dimension
                (N, C, H, W) where C = input_channels from the
                initialization

        Returns:
            X_hat: the result of both propagations
        &#34;&#34;&#34;
        gen_img = self.znet(dense)
        gen_img = self.dec(gen_img)

        return gen_img

    def decode(self, latent: torch.FloatTensor) -&gt; torch.FloatTensor:
        &#34;&#34;&#34;Decode a vector of latent features.

        Args:
            latent: a torch tensor of dimension (N, C, H, W)
                where C = 512 from latent features

        Returns:
            dec_img: resulting image via decoding
        &#34;&#34;&#34;
        dec_img = self.dec(latent)

        return dec_img
        </code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="src.gan.generator.Dec"><code class="flex name class">
<span>class <span class="ident">Dec</span></span>
</code></dt>
<dd>
<div class="desc"><p>Implement the Decoder part of the Generator.</p>
<p>Initialize the Decoder (or Dec).</p>
<p>This is the second part of the Generator. Hence, its task is
to work with the latent code and generate a fake image to be sent
to the Discriminator</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Dec(nn.Module):
    &#34;&#34;&#34;Implement the Decoder part of the Generator.&#34;&#34;&#34;

    def __init__(self):
        &#34;&#34;&#34;Initialize the Decoder (or Dec).

        This is the second part of the Generator. Hence, its task is
        to work with the latent code and generate a fake image to be sent
        to the Discriminator
        &#34;&#34;&#34;
        super(Dec, self).__init__()

        self.deconv3 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=4, stride=2, padding=1)
        self.deconv3bn = nn.BatchNorm2d(num_features=256, affine=True)
        self.deconv3dropout = nn.Dropout(p=0.5)
        self.deconv3relu = nn.ReLU(inplace=False)

        self.deconv4 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1)
        self.deconv4bn = nn.BatchNorm2d(num_features=128, affine=True)
        self.deconv4dropout = nn.Dropout(p=0.5)
        self.deconv4relu = nn.ReLU(inplace=False)

        self.deconv5 = nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)
        self.deconv5bn = nn.BatchNorm2d(num_features=128, affine=True)
        self.deconv5relu = nn.ReLU(inplace=False)

        self.deconv6 = nn.ConvTranspose2d(in_channels=128, out_channels=3, kernel_size=4, stride=2, padding=1)
        self.deconv6sig = nn.Sigmoid()

    def forward(self, latent: torch.FloatTensor) -&gt; torch.FloatTensor:
        &#34;&#34;&#34;Compute the forward propagation of this network on input data.

        Args:
            latent: a torch tensor of dimension (N, C, H, W)
                to be treated

        Returns:
            dec_img: the result of the propagation with this network
        &#34;&#34;&#34;
        dec_img = self.deconv3(latent)
        dec_img = self.deconv3bn(dec_img)
        dec_img = self.deconv3dropout(dec_img)
        dec_img = self.deconv3relu(dec_img)

        dec_img = self.deconv4(dec_img)
        dec_img = self.deconv4bn(dec_img)
        dec_img = self.deconv4dropout(dec_img)
        dec_img = self.deconv4relu(dec_img)

        dec_img = self.deconv5(dec_img)
        dec_img = self.deconv5bn(dec_img)
        dec_img = self.deconv5relu(dec_img)

        dec_img = self.deconv6(dec_img)
        dec_img = self.deconv6sig(dec_img)

        return dec_img</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="src.gan.generator.Dec.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="src.gan.generator.Dec.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="src.gan.generator.Dec.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, latent: torch.FloatTensor) ‑> torch.FloatTensor</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the forward propagation of this network on input data.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>latent</code></strong></dt>
<dd>a torch tensor of dimension (N, C, H, W)
to be treated</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dec_img</code></dt>
<dd>the result of the propagation with this network</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, latent: torch.FloatTensor) -&gt; torch.FloatTensor:
    &#34;&#34;&#34;Compute the forward propagation of this network on input data.

    Args:
        latent: a torch tensor of dimension (N, C, H, W)
            to be treated

    Returns:
        dec_img: the result of the propagation with this network
    &#34;&#34;&#34;
    dec_img = self.deconv3(latent)
    dec_img = self.deconv3bn(dec_img)
    dec_img = self.deconv3dropout(dec_img)
    dec_img = self.deconv3relu(dec_img)

    dec_img = self.deconv4(dec_img)
    dec_img = self.deconv4bn(dec_img)
    dec_img = self.deconv4dropout(dec_img)
    dec_img = self.deconv4relu(dec_img)

    dec_img = self.deconv5(dec_img)
    dec_img = self.deconv5bn(dec_img)
    dec_img = self.deconv5relu(dec_img)

    dec_img = self.deconv6(dec_img)
    dec_img = self.deconv6sig(dec_img)

    return dec_img</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="src.gan.generator.Generator"><code class="flex name class">
<span>class <span class="ident">Generator</span></span>
<span>(</span><span>img_size: int, start_channels: int = 110)</span>
</code></dt>
<dd>
<div class="desc"><p>Gather the two parts of the Generator in one class.</p>
<p>Initialize the complete generator part of the GAN.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>img_size</code></strong></dt>
<dd>the size of the output images to be built
and sent to the Discriminator, which are expected to be
squared images of size img_size<em>img_size</em>input_channels
where input_channels is used for the Discriminator's
initialization</dd>
<dt><strong><code>start_channels</code></strong></dt>
<dd>the number of channels in the
input images, free to choose</dd>
</dl>
<h2 id="raises">Raises</h2>
<p>ValueError if the value of img_size is less than 64 pixels due
to the operations executed by the deconv layers</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Generator(nn.Module):
    &#34;&#34;&#34;Gather the two parts of the Generator in one class.&#34;&#34;&#34;

    def __init__(self, img_size: int, start_channels: int = 110):
        &#34;&#34;&#34;Initialize the complete generator part of the GAN.

        Args:
            img_size: the size of the output images to be built
                and sent to the Discriminator, which are expected to be
                squared images of size img_size*img_size*input_channels
                where input_channels is used for the Discriminator&#39;s
                initialization
            start_channels: the number of channels in the
                input images, free to choose

        Raises:
            ValueError if the value of img_size is less than 64 pixels due
            to the operations executed by the deconv layers
        &#34;&#34;&#34;
        if img_size &lt; 64:
            raise ValueError(&#34;The size of the input images has to be of at least 64 pixels!&#34;)

        super(Generator, self).__init__()

        self.znet = zNet(img_size, start_channels=start_channels)
        self.dec = Dec()

    def forward(self, dense: torch.FloatTensor) -&gt; torch.FloatTensor:
        &#34;&#34;&#34;Compute the forward propagation on input data.

        Args:
            dense: a torch tensor of dimension
                (N, C, H, W) where C = input_channels from the
                initialization

        Returns:
            X_hat: the result of both propagations
        &#34;&#34;&#34;
        gen_img = self.znet(dense)
        gen_img = self.dec(gen_img)

        return gen_img

    def decode(self, latent: torch.FloatTensor) -&gt; torch.FloatTensor:
        &#34;&#34;&#34;Decode a vector of latent features.

        Args:
            latent: a torch tensor of dimension (N, C, H, W)
                where C = 512 from latent features

        Returns:
            dec_img: resulting image via decoding
        &#34;&#34;&#34;
        dec_img = self.dec(latent)

        return dec_img</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="src.gan.generator.Generator.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="src.gan.generator.Generator.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="src.gan.generator.Generator.decode"><code class="name flex">
<span>def <span class="ident">decode</span></span>(<span>self, latent: torch.FloatTensor) ‑> torch.FloatTensor</span>
</code></dt>
<dd>
<div class="desc"><p>Decode a vector of latent features.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>latent</code></strong></dt>
<dd>a torch tensor of dimension (N, C, H, W)
where C = 512 from latent features</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dec_img</code></dt>
<dd>resulting image via decoding</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def decode(self, latent: torch.FloatTensor) -&gt; torch.FloatTensor:
    &#34;&#34;&#34;Decode a vector of latent features.

    Args:
        latent: a torch tensor of dimension (N, C, H, W)
            where C = 512 from latent features

    Returns:
        dec_img: resulting image via decoding
    &#34;&#34;&#34;
    dec_img = self.dec(latent)

    return dec_img</code></pre>
</details>
</dd>
<dt id="src.gan.generator.Generator.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, dense: torch.FloatTensor) ‑> torch.FloatTensor</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the forward propagation on input data.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dense</code></strong></dt>
<dd>a torch tensor of dimension
(N, C, H, W) where C = input_channels from the
initialization</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>X_hat</code></dt>
<dd>the result of both propagations</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, dense: torch.FloatTensor) -&gt; torch.FloatTensor:
    &#34;&#34;&#34;Compute the forward propagation on input data.

    Args:
        dense: a torch tensor of dimension
            (N, C, H, W) where C = input_channels from the
            initialization

    Returns:
        X_hat: the result of both propagations
    &#34;&#34;&#34;
    gen_img = self.znet(dense)
    gen_img = self.dec(gen_img)

    return gen_img</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="src.gan.generator.zNet"><code class="flex name class">
<span>class <span class="ident">zNet</span></span>
<span>(</span><span>img_size: int, start_channels: int = 110)</span>
</code></dt>
<dd>
<div class="desc"><p>Implement the module converting dense features to latent features.</p>
<p>Initialize the zNet network.</p>
<p>This is the first part of the Generator. Hence, its task is
to convert the dense code into a latent code to be used in the
Decoder</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>img_size</code></strong></dt>
<dd>the size of the output images to be created
at the end of the Generator, which are expected to be
squared images of size img_size<em>img_size</em>input_channels where
input_channels is from the Discriminator initialization (not
this one)</dd>
<dt><strong><code>start_channels</code></strong></dt>
<dd>the number of channels in
the input data</dd>
</dl>
<h2 id="raises">Raises</h2>
<p>ValueError if the size of image is less than 64 due to
the operations executed by the deconv layers</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class zNet(nn.Module):
    &#34;&#34;&#34;Implement the module converting dense features to latent features.&#34;&#34;&#34;

    def __init__(self, img_size: int, start_channels: int = 110):
        &#34;&#34;&#34;Initialize the zNet network.

        This is the first part of the Generator. Hence, its task is
        to convert the dense code into a latent code to be used in the
        Decoder

        Args:
            img_size: the size of the output images to be created
                at the end of the Generator, which are expected to be
                squared images of size img_size*img_size*input_channels where
                input_channels is from the Discriminator initialization (not
                this one)
            start_channels: the number of channels in
                the input data

        Raises:
            ValueError if the size of image is less than 64 due to
            the operations executed by the deconv layers
        &#34;&#34;&#34;
        super(zNet, self).__init__()

        # Parameters

        if img_size &lt; 64:
            raise ValueError(&#34;The size of the input images has to be of at least 64 pixels!&#34;)

        self.start_channels = start_channels
        self.img_size = img_size

        # Deconvolution layers

        self.deconv1 = nn.ConvTranspose2d(in_channels=start_channels, out_channels=1024, kernel_size=4, stride=1, padding=0)
        self.deconv1bn = nn.BatchNorm2d(num_features=1024, affine=True)
        self.deconv1relu = nn.ReLU(inplace=False)

        self.deconv2 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=4, stride=2, padding=1)
        self.deconv2bn = nn.BatchNorm2d(num_features=512, affine=True)
        self.deconv2relu = nn.ReLU(inplace=False)

    def forward(self, dense: torch.FloatTensor) -&gt; torch.FloatTensor:
        &#34;&#34;&#34;Compute the propagation of this network on input data.

        Args:
            dense: a torch tensor of dimension
                (N, C, H, W)

        Returns:
            latent: the result of propagation with this
                network
        &#34;&#34;&#34;
        # We need to ensure that the data has correct size, for this we used the formula to
        # calculate the result of convolutions (reversed)
        latent = dense.view(dense.size(0), self.start_channels, (self.img_size//16)-3, (self.img_size//16)-3)
        latent = self.deconv1(latent)
        latent = self.deconv1bn(latent)
        latent = self.deconv1relu(latent)

        latent = self.deconv2(latent)
        latent = self.deconv2bn(latent)
        latent = self.deconv2relu(latent)

        return latent</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="src.gan.generator.zNet.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="src.gan.generator.zNet.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="src.gan.generator.zNet.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, dense: torch.FloatTensor) ‑> torch.FloatTensor</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the propagation of this network on input data.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dense</code></strong></dt>
<dd>a torch tensor of dimension
(N, C, H, W)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>latent</code></dt>
<dd>the result of propagation with this
network</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, dense: torch.FloatTensor) -&gt; torch.FloatTensor:
    &#34;&#34;&#34;Compute the propagation of this network on input data.

    Args:
        dense: a torch tensor of dimension
            (N, C, H, W)

    Returns:
        latent: the result of propagation with this
            network
    &#34;&#34;&#34;
    # We need to ensure that the data has correct size, for this we used the formula to
    # calculate the result of convolutions (reversed)
    latent = dense.view(dense.size(0), self.start_channels, (self.img_size//16)-3, (self.img_size//16)-3)
    latent = self.deconv1(latent)
    latent = self.deconv1bn(latent)
    latent = self.deconv1relu(latent)

    latent = self.deconv2(latent)
    latent = self.deconv2bn(latent)
    latent = self.deconv2relu(latent)

    return latent</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src.gan" href="index.html">src.gan</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="src.gan.generator.Dec" href="#src.gan.generator.Dec">Dec</a></code></h4>
<ul class="">
<li><code><a title="src.gan.generator.Dec.dump_patches" href="#src.gan.generator.Dec.dump_patches">dump_patches</a></code></li>
<li><code><a title="src.gan.generator.Dec.forward" href="#src.gan.generator.Dec.forward">forward</a></code></li>
<li><code><a title="src.gan.generator.Dec.training" href="#src.gan.generator.Dec.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.gan.generator.Generator" href="#src.gan.generator.Generator">Generator</a></code></h4>
<ul class="">
<li><code><a title="src.gan.generator.Generator.decode" href="#src.gan.generator.Generator.decode">decode</a></code></li>
<li><code><a title="src.gan.generator.Generator.dump_patches" href="#src.gan.generator.Generator.dump_patches">dump_patches</a></code></li>
<li><code><a title="src.gan.generator.Generator.forward" href="#src.gan.generator.Generator.forward">forward</a></code></li>
<li><code><a title="src.gan.generator.Generator.training" href="#src.gan.generator.Generator.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.gan.generator.zNet" href="#src.gan.generator.zNet">zNet</a></code></h4>
<ul class="">
<li><code><a title="src.gan.generator.zNet.dump_patches" href="#src.gan.generator.zNet.dump_patches">dump_patches</a></code></li>
<li><code><a title="src.gan.generator.zNet.forward" href="#src.gan.generator.zNet.forward">forward</a></code></li>
<li><code><a title="src.gan.generator.zNet.training" href="#src.gan.generator.zNet.training">training</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>