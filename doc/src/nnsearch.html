<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>src.nnsearch API documentation</title>
<meta name="description" content="Evaluate the ArtGAN using k-NN search â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.nnsearch</code></h1>
</header>
<section id="section-intro">
<p>Evaluate the ArtGAN using k-NN search.</p>
<p>This is a file you want to run from root folder with
specified dataset and options.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Evaluate the ArtGAN using k-NN search.

This is a file you want to run from root folder with
specified dataset and options.
&#34;&#34;&#34;


# Importing Python packages
import os
from typing import List
import argparse
import json
import heapq
from tqdm import tqdm
import torch
from torch.utils.data import Dataset
import torchvision
import torchvision.transforms as transforms
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import ImageGrid

# Importing our own files and classes
from gan.artgan import ArtGAN
from datasets.wikiart import Wikiart


def knnShow(k : int, n : int, starting_epoch: int, ending_epoch: int,  step_epoch: int,
            device: torch.device, use_cuda: bool,
            data_type:str, version: str, data_classes: List,
            start_channels: int, img_size: int,
            nb_classes: int, trainset: Dataset) -&gt; None:
    &#34;&#34;&#34;Plot the k-NN search for the given period of time.

    Args:
        k: the number of neighbors
        n: the number of images per class
        starting_epoch: the epoch to start with
        ending_epoch: the epoch at which the algorithm ends
        step_epoch: the step between lines
        device: the device to use everywhere
        use_cuda: whether we want to use cuda here
        data_type: the name of the dataset
        version: the name of the ArtGAN version
        data_classes: a list of the names of the classes
            in the dataset
        start_channels: the number of starting channels of
            the Generator
        img_size: the size of the input images
        nb_classes: the number of classes in the dataset
        trainset: the dataset itself
    &#34;&#34;&#34;
    Z_hat = torch.randn(n*nb_classes, start_channels-nb_classes, device=device)
    Yk_hat = torch.zeros(n*nb_classes, nb_classes, device=device)
    for i in range(nb_classes):
        for j in range(n):
            Yk_hat[i*n +j][i] = 1
    Z_Yk_fixed = torch.cat([Z_hat, Yk_hat], dim=1)
    img_labels = torch.argmax(Yk_hat, dim=1)
    T = [i for i in range(starting_epoch, ending_epoch+1, step_epoch)]

    images = []

    epoch_pbar = tqdm(T, desc=&#34;Epoch: {}&#34;.format(T[0]))
    for epoch in epoch_pbar:
        epoch_pbar.set_description(&#34;Epoch: {}&#34;.format(epoch))
        epochImages = []

        step_artgan = ArtGAN(data_type, version, img_size, nb_classes,
                            start_channels=start_channels, retrain_epoch=epoch,
                            device=device)

        if use_cuda: step_artgan.cuda()
        step_artgan.eval()

        model_imgs = step_artgan.G(Z_Yk_fixed)
        model_imgs = model_imgs.cpu().detach()

        nn_pbar = tqdm(range(nb_classes), desc=&#34;Label: {}&#34;.format(data_classes[0]))
        for i in nn_pbar:
            nn_pbar.set_description(&#34;Label: {}&#34;.format(data_classes[i]))
            for j in range(n):
                gen_img = model_imgs[i*n+j]
                neighbors = knn(k , gen_img, trainset)
                neighbors = [trainset.__getitem__(idx)[0].numpy() for idx in neighbors]
                gen_img = np.transpose(gen_img, (1, 2, 0))
                neighbors = [ np.transpose(img, (1, 2, 0)) for img in neighbors]
                l = [gen_img] + neighbors
                epochImages.append(l)

        images.append(epochImages)

    for j in range(n*nb_classes):
        ncols = k+1
        nrows = len(T)

        fig = plt.figure(figsize=(4, 4))
        grid = ImageGrid(fig, 111, nrows_ncols=(nrows, ncols), axes_pad=(0.05, 0.3))

        row = 0
        col = 0
        for ax in grid:

            ax.imshow(images[row][j][col])
            ax.axis(&#34;off&#34;)
            if col == 0:
                ax.set_title(&#34;Epoch {}&#34;.format(T[row]))

            col += 1
            col = col%ncols
            if col == 0: row += 1

        # fig.suptitle(&#34;Generation of {} at different epochs&#34;.format(data_classes[img_labels[j]]))
        plt.tight_layout()
        eval_folder = &#34;results/&#34; + data_type + &#34;_&#34; + version + &#34;/knn/&#34;
        if not os.path.exists(eval_folder):
            os.makedirs(eval_folder)
        path_to_file = eval_folder + &#34;neighbor_evo_{}.png&#34;.format(data_classes[img_labels[j]])
        plt.savefig(path_to_file)
        plt.close()

def knn(k: int, genImg: torch.FloatTensor, trainData: Dataset) -&gt; List[torch.FloatTensor]:
    &#34;&#34;&#34;Compute k-NN search on given images.

    Args:
        k: the number of neighbors
        genImg: the images generated by the Generator
        trainData: the dataset itself

    Returns:
        res: the nearest neighbors of specified images
    &#34;&#34;&#34;
    lib = {}
    h = []
    for i in range(trainData.__len__()):
        ele, _ = trainData.__getitem__(i)

        v = torch.sqrt(torch.sum( torch.square(ele -genImg) ))
        lib[v] = i
        heapq.heappush(h,v)
    resVal = [heapq.heappop(h) for i in range(k)]
    res = [lib[val] for val in resVal]

    return res


if __name__ == &#34;__main__&#34;:

    # Command lines
    parser = argparse.ArgumentParser(description=&#34;Main file to train and evaluate ArtGAN.&#34;)
    parser.add_argument(&#34;data_type&#34;, type=str, help=&#34;Please choose a dataset from those supported.&#34;)
    parser.add_argument(&#34;-v&#34;, &#34;--version&#34;, type=str, help=&#34;Please choose a version for saving results. Default: temp.&#34;)
    parser.add_argument(&#34;-a&#34;, &#34;--startepoch&#34;, type=int, help=&#34;Please choose a starting epoch. Default: 5.&#34;)
    parser.add_argument(&#34;-e&#34;, &#34;--endepoch&#34;, type=int, help=&#34;Please choose an ending epoch. Default: 5.&#34;)
    parser.add_argument(&#34;-s&#34;, &#34;--stepepoch&#34;, type=int, help=&#34;Type the amount of epochs to step between lines. Default: 1.&#34;)
    parser.add_argument(&#34;-n&#34;, &#34;--n&#34;, type=int, help=&#34;Type the amount of images to generate for each class of images. Default: 1.&#34;)
    parser.add_argument(&#34;-k&#34;, &#34;--kneighbors&#34;, type=int, help=&#34;Type the amount of nearest neighbours you wish to find. Default: 1.&#34;)
    # parser.add_argument(&#34;-d&#34;, &#34;--distance&#34;, type=int, help=&#34;Type any number if you want to save distance in a file. Default: False.&#34;)
    args = parser.parse_args()

    data_type = args.data_type
    version = args.version if args.version else &#34;temp&#34;
    startEpoch = args.startepoch if args.startepoch else 5
    endEpoch = args.endepoch if args.endepoch else 5
    stepEpoch = args.stepepoch if args.stepepoch else 1
    n = args.n if args.n else 1
    k = args.kneighbors if args.kneighbors else 1
    # save_distance = True if args.distance else False


    # Turning on CUDA globally
    USE_CUDA = torch.cuda.is_available()
    print(&#34;Will we use CUDA? {}&#34;.format(USE_CUDA))
    DEVICE = torch.device(&#34;cuda&#34; if USE_CUDA else &#34;cpu&#34;)

    # Global parameters for our GANs
    batch_size = 128
    img_size = 64

    if data_type == &#34;cifar&#34;:

        with open(&#34;src/datasets/cifar.json&#34;, &#34;r&#34;) as f:
            CIFAR10_CLASSES = json.load(f)

        transform = transforms.Compose([
                                        transforms.Resize(64),
                                        transforms.ToTensor(),
                                        ])

        trainset = torchvision.datasets.CIFAR10(root=&#34;data/&#34;, train=True,
                                                download=True, transform=transform)

        data_classes = CIFAR10_CLASSES
        nb_classes = len(data_classes)
        data_classes.append(&#34;FAKE&#34;)

    elif data_type in {&#34;artist&#34;, &#34;genre&#34;, &#34;style&#34;}:

        with open(&#34;src/datasets/wikiart.json&#34;, &#34;r&#34;) as f:
            WIKIART_CLASSES = json.load(f)

        # Data is expected to be resized prior, in order to accelerate training
        transform = transforms.Compose([
                                        transforms.ToTensor(),
                                        ])

        trainset = Wikiart(data_type, transform, train=True)

        data_classes = WIKIART_CLASSES[data_type]
        nb_classes = len(data_classes)
        data_classes.append(&#34;FAKE&#34;)

    else:

        raise ValueError(&#34;This dataset is not supported!&#34;)

    # GAN parameters
    start_channels = 100 + nb_classes

    knnShow(k,n,startEpoch,endEpoch, stepEpoch,DEVICE,USE_CUDA,data_type, version, data_classes,start_channels,img_size,nb_classes, trainset)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="src.nnsearch.knn"><code class="name flex">
<span>def <span class="ident">knn</span></span>(<span>k:Â int, genImg:Â torch.FloatTensor, trainData:Â torch.utils.data.dataset.Dataset) â€‘>Â List[torch.FloatTensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Compute k-NN search on given images.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>k</code></strong></dt>
<dd>the number of neighbors</dd>
<dt><strong><code>genImg</code></strong></dt>
<dd>the images generated by the Generator</dd>
<dt><strong><code>trainData</code></strong></dt>
<dd>the dataset itself</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>res</code></dt>
<dd>the nearest neighbors of specified images</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def knn(k: int, genImg: torch.FloatTensor, trainData: Dataset) -&gt; List[torch.FloatTensor]:
    &#34;&#34;&#34;Compute k-NN search on given images.

    Args:
        k: the number of neighbors
        genImg: the images generated by the Generator
        trainData: the dataset itself

    Returns:
        res: the nearest neighbors of specified images
    &#34;&#34;&#34;
    lib = {}
    h = []
    for i in range(trainData.__len__()):
        ele, _ = trainData.__getitem__(i)

        v = torch.sqrt(torch.sum( torch.square(ele -genImg) ))
        lib[v] = i
        heapq.heappush(h,v)
    resVal = [heapq.heappop(h) for i in range(k)]
    res = [lib[val] for val in resVal]

    return res</code></pre>
</details>
</dd>
<dt id="src.nnsearch.knnShow"><code class="name flex">
<span>def <span class="ident">knnShow</span></span>(<span>k:Â int, n:Â int, starting_epoch:Â int, ending_epoch:Â int, step_epoch:Â int, device:Â torch.device, use_cuda:Â bool, data_type:Â str, version:Â str, data_classes:Â List, start_channels:Â int, img_size:Â int, nb_classes:Â int, trainset:Â torch.utils.data.dataset.Dataset) â€‘>Â NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Plot the k-NN search for the given period of time.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>k</code></strong></dt>
<dd>the number of neighbors</dd>
<dt><strong><code>n</code></strong></dt>
<dd>the number of images per class</dd>
<dt><strong><code>starting_epoch</code></strong></dt>
<dd>the epoch to start with</dd>
<dt><strong><code>ending_epoch</code></strong></dt>
<dd>the epoch at which the algorithm ends</dd>
<dt><strong><code>step_epoch</code></strong></dt>
<dd>the step between lines</dd>
<dt><strong><code>device</code></strong></dt>
<dd>the device to use everywhere</dd>
<dt><strong><code>use_cuda</code></strong></dt>
<dd>whether we want to use cuda here</dd>
<dt><strong><code>data_type</code></strong></dt>
<dd>the name of the dataset</dd>
<dt><strong><code>version</code></strong></dt>
<dd>the name of the ArtGAN version</dd>
<dt><strong><code>data_classes</code></strong></dt>
<dd>a list of the names of the classes
in the dataset</dd>
<dt><strong><code>start_channels</code></strong></dt>
<dd>the number of starting channels of
the Generator</dd>
<dt><strong><code>img_size</code></strong></dt>
<dd>the size of the input images</dd>
<dt><strong><code>nb_classes</code></strong></dt>
<dd>the number of classes in the dataset</dd>
<dt><strong><code>trainset</code></strong></dt>
<dd>the dataset itself</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def knnShow(k : int, n : int, starting_epoch: int, ending_epoch: int,  step_epoch: int,
            device: torch.device, use_cuda: bool,
            data_type:str, version: str, data_classes: List,
            start_channels: int, img_size: int,
            nb_classes: int, trainset: Dataset) -&gt; None:
    &#34;&#34;&#34;Plot the k-NN search for the given period of time.

    Args:
        k: the number of neighbors
        n: the number of images per class
        starting_epoch: the epoch to start with
        ending_epoch: the epoch at which the algorithm ends
        step_epoch: the step between lines
        device: the device to use everywhere
        use_cuda: whether we want to use cuda here
        data_type: the name of the dataset
        version: the name of the ArtGAN version
        data_classes: a list of the names of the classes
            in the dataset
        start_channels: the number of starting channels of
            the Generator
        img_size: the size of the input images
        nb_classes: the number of classes in the dataset
        trainset: the dataset itself
    &#34;&#34;&#34;
    Z_hat = torch.randn(n*nb_classes, start_channels-nb_classes, device=device)
    Yk_hat = torch.zeros(n*nb_classes, nb_classes, device=device)
    for i in range(nb_classes):
        for j in range(n):
            Yk_hat[i*n +j][i] = 1
    Z_Yk_fixed = torch.cat([Z_hat, Yk_hat], dim=1)
    img_labels = torch.argmax(Yk_hat, dim=1)
    T = [i for i in range(starting_epoch, ending_epoch+1, step_epoch)]

    images = []

    epoch_pbar = tqdm(T, desc=&#34;Epoch: {}&#34;.format(T[0]))
    for epoch in epoch_pbar:
        epoch_pbar.set_description(&#34;Epoch: {}&#34;.format(epoch))
        epochImages = []

        step_artgan = ArtGAN(data_type, version, img_size, nb_classes,
                            start_channels=start_channels, retrain_epoch=epoch,
                            device=device)

        if use_cuda: step_artgan.cuda()
        step_artgan.eval()

        model_imgs = step_artgan.G(Z_Yk_fixed)
        model_imgs = model_imgs.cpu().detach()

        nn_pbar = tqdm(range(nb_classes), desc=&#34;Label: {}&#34;.format(data_classes[0]))
        for i in nn_pbar:
            nn_pbar.set_description(&#34;Label: {}&#34;.format(data_classes[i]))
            for j in range(n):
                gen_img = model_imgs[i*n+j]
                neighbors = knn(k , gen_img, trainset)
                neighbors = [trainset.__getitem__(idx)[0].numpy() for idx in neighbors]
                gen_img = np.transpose(gen_img, (1, 2, 0))
                neighbors = [ np.transpose(img, (1, 2, 0)) for img in neighbors]
                l = [gen_img] + neighbors
                epochImages.append(l)

        images.append(epochImages)

    for j in range(n*nb_classes):
        ncols = k+1
        nrows = len(T)

        fig = plt.figure(figsize=(4, 4))
        grid = ImageGrid(fig, 111, nrows_ncols=(nrows, ncols), axes_pad=(0.05, 0.3))

        row = 0
        col = 0
        for ax in grid:

            ax.imshow(images[row][j][col])
            ax.axis(&#34;off&#34;)
            if col == 0:
                ax.set_title(&#34;Epoch {}&#34;.format(T[row]))

            col += 1
            col = col%ncols
            if col == 0: row += 1

        # fig.suptitle(&#34;Generation of {} at different epochs&#34;.format(data_classes[img_labels[j]]))
        plt.tight_layout()
        eval_folder = &#34;results/&#34; + data_type + &#34;_&#34; + version + &#34;/knn/&#34;
        if not os.path.exists(eval_folder):
            os.makedirs(eval_folder)
        path_to_file = eval_folder + &#34;neighbor_evo_{}.png&#34;.format(data_classes[img_labels[j]])
        plt.savefig(path_to_file)
        plt.close()</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src" href="index.html">src</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="src.nnsearch.knn" href="#src.nnsearch.knn">knn</a></code></li>
<li><code><a title="src.nnsearch.knnShow" href="#src.nnsearch.knnShow">knnShow</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>